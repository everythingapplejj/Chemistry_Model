{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e8108e-d408-48d9-8aee-c9b1fd6597e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.diffusion.distributions import DistributionNodes\n",
    "import src.utils as utils\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data.lightning import LightningDataset\n",
    "\n",
    "\n",
    "class AbstractDataModule(LightningDataset):\n",
    "    def __init__(self, cfg, datasets):\n",
    "        super().__init__(train_dataset=datasets['train'], val_dataset=datasets['val'], test_dataset=datasets['test'],\n",
    "                         batch_size=cfg.train.batch_size if 'debug' not in cfg.general.name else 2,\n",
    "                         num_workers=cfg.train.num_workers,\n",
    "                         pin_memory=getattr(cfg.dataset, \"pin_memory\", False))\n",
    "        self.cfg = cfg\n",
    "        self.input_dims = None\n",
    "        self.output_dims = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_dataset[idx]\n",
    "\n",
    "    def node_counts(self, max_nodes_possible=300):\n",
    "        all_counts = torch.zeros(max_nodes_possible)\n",
    "        for loader in [self.train_dataloader(), self.val_dataloader()]:\n",
    "            for data in loader:\n",
    "                unique, counts = torch.unique(data.batch, return_counts=True)\n",
    "                for count in counts:\n",
    "                    all_counts[count] += 1\n",
    "        max_index = max(all_counts.nonzero())\n",
    "        all_counts = all_counts[:max_index + 1]\n",
    "        all_counts = all_counts / all_counts.sum()\n",
    "        return all_counts\n",
    "\n",
    "    def node_types(self):\n",
    "        num_classes = None\n",
    "        for data in self.train_dataloader():\n",
    "            num_classes = data.x.shape[1]\n",
    "            break\n",
    "\n",
    "        counts = torch.zeros(num_classes)\n",
    "\n",
    "        for i, data in enumerate(self.train_dataloader()):\n",
    "            counts += data.x.sum(dim=0)\n",
    "\n",
    "        counts = counts / counts.sum()\n",
    "        return counts\n",
    "\n",
    "    def edge_counts(self):\n",
    "        num_classes = None\n",
    "        for data in self.train_dataloader():\n",
    "            num_classes = data.edge_attr.shape[1]\n",
    "            break\n",
    "\n",
    "        d = torch.zeros(num_classes, dtype=torch.float)\n",
    "\n",
    "        for i, data in enumerate(self.train_dataloader()):\n",
    "            unique, counts = torch.unique(data.batch, return_counts=True)\n",
    "\n",
    "            all_pairs = 0\n",
    "            for count in counts:\n",
    "                all_pairs += count * (count - 1)\n",
    "\n",
    "            num_edges = data.edge_index.shape[1]\n",
    "            num_non_edges = all_pairs - num_edges\n",
    "\n",
    "            edge_types = data.edge_attr.sum(dim=0)\n",
    "            assert num_non_edges >= 0\n",
    "            d[0] += num_non_edges\n",
    "            d[1:] += edge_types[1:]\n",
    "\n",
    "        d = d / d.sum()\n",
    "        return d\n",
    "\n",
    "\n",
    "class MolecularDataModule(AbstractDataModule):\n",
    "    def valency_count(self, max_n_nodes):\n",
    "        valencies = torch.zeros(3 * max_n_nodes - 2)   # Max valency possible if everything is connected\n",
    "\n",
    "        # No bond, single bond, double bond, triple bond, aromatic bond\n",
    "        multiplier = torch.tensor([0, 1, 2, 3, 1.5])\n",
    "\n",
    "        for data in self.train_dataloader():\n",
    "            n = data.x.shape[0]\n",
    "\n",
    "            for atom in range(n):\n",
    "                edges = data.edge_attr[data.edge_index[0] == atom]\n",
    "                edges_total = edges.sum(dim=0)\n",
    "                valency = (edges_total * multiplier).sum()\n",
    "                valencies[valency.long().item()] += 1\n",
    "        valencies = valencies / valencies.sum()\n",
    "        return valencies\n",
    "\n",
    "\n",
    "class AbstractDatasetInfos:\n",
    "    def complete_infos(self, n_nodes, node_types):\n",
    "        self.input_dims = None\n",
    "        self.output_dims = None\n",
    "        self.num_classes = len(node_types)\n",
    "        self.max_n_nodes = len(n_nodes) - 1\n",
    "        self.nodes_dist = DistributionNodes(n_nodes)\n",
    "\n",
    "    def compute_input_output_dims(self, datamodule, extra_features, domain_features):\n",
    "        example_batch = next(iter(datamodule.train_dataloader()))\n",
    "        ex_dense, node_mask = utils.to_dense(example_batch.x, example_batch.edge_index, example_batch.edge_attr,\n",
    "                                             example_batch.batch)\n",
    "        example_data = {'X_t': ex_dense.X, 'E_t': ex_dense.E, 'y_t': example_batch['y'], 'node_mask': node_mask}\n",
    "\n",
    "        self.input_dims = {'X': example_batch['x'].size(1),\n",
    "                           'E': example_batch['edge_attr'].size(1),\n",
    "                           'y': example_batch['y'].size(1) + 1}      # + 1 due to time conditioning\n",
    "        ex_extra_feat = extra_features(example_data)\n",
    "        self.input_dims['X'] += ex_extra_feat.X.size(-1)\n",
    "        self.input_dims['E'] += ex_extra_feat.E.size(-1)\n",
    "        self.input_dims['y'] += ex_extra_feat.y.size(-1)\n",
    "\n",
    "        ex_extra_molecular_feat = domain_features(example_data)\n",
    "        self.input_dims['X'] += ex_extra_molecular_feat.X.size(-1)\n",
    "        self.input_dims['E'] += ex_extra_molecular_feat.E.size(-1)\n",
    "        self.input_dims['y'] += ex_extra_molecular_feat.y.size(-1)\n",
    "\n",
    "        self.output_dims = {'X': example_batch['x'].size(1),\n",
    "                            'E': example_batch['edge_attr'].size(1),\n",
    "                            'y': 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90a9b682-07ee-4608-891a-f8d0e0d173b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "from typing import Any, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import pandas as pd\n",
    "\n",
    "from src import utils\n",
    "from src.analysis.rdkit_functions import mol2smiles, build_molecule_with_partial_charges, compute_molecular_metrics\n",
    "from src.datasets.abstract_dataset import AbstractDatasetInfos, MolecularDataModule\n",
    "\n",
    "\n",
    "def to_list(value: Any) -> Sequence:\n",
    "    if isinstance(value, Sequence) and not isinstance(value, str):\n",
    "        return value\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "\n",
    "atom_decoder = ['C', 'N', 'S', 'O', 'F', 'Cl', 'Br', 'H']\n",
    "\n",
    "\n",
    "class MOSESDataset(InMemoryDataset):\n",
    "    train_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/train.csv'\n",
    "    val_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/test.csv'\n",
    "    test_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/test_scaffolds.csv'\n",
    "\n",
    "    def __init__(self, stage, root, filter_dataset: bool, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.stage = stage\n",
    "        self.atom_decoder = atom_decoder\n",
    "        self.filter_dataset = filter_dataset\n",
    "        if self.stage == 'train':\n",
    "            self.file_idx = 0\n",
    "        elif self.stage == 'val':\n",
    "            self.file_idx = 1\n",
    "        else:\n",
    "            self.file_idx = 2\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.file_idx])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train_moses.csv', 'val_moses.csv', 'test_moses.csv']\n",
    "\n",
    "    @property\n",
    "    def split_file_name(self):\n",
    "        return ['train_moses.csv', 'val_moses.csv', 'test_moses.csv']\n",
    "\n",
    "    @property\n",
    "    def split_paths(self):\n",
    "        r\"\"\"The absolute filepaths that must be present in order to skip\n",
    "        splitting.\"\"\"\n",
    "        files = to_list(self.split_file_name)\n",
    "        return [osp.join(self.raw_dir, f) for f in files]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.filter_dataset:\n",
    "            return ['train_filtered.pt', 'test_filtered.pt', 'test_scaffold_filtered.pt']\n",
    "        else:\n",
    "            return ['train.pt', 'test.pt', 'test_scaffold.pt']\n",
    "\n",
    "    def download(self):\n",
    "        import rdkit  # noqa\n",
    "        train_path = download_url(self.train_url, self.raw_dir)\n",
    "        os.rename(train_path, osp.join(self.raw_dir, 'train_moses.csv'))\n",
    "\n",
    "        test_path = download_url(self.test_url, self.raw_dir)\n",
    "        os.rename(test_path, osp.join(self.raw_dir, 'val_moses.csv'))\n",
    "\n",
    "        valid_path = download_url(self.val_url, self.raw_dir)\n",
    "        os.rename(valid_path, osp.join(self.raw_dir, 'test_moses.csv'))\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        RDLogger.DisableLog('rdApp.*')\n",
    "        types = {atom: i for i, atom in enumerate(self.atom_decoder)}\n",
    "\n",
    "        bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "\n",
    "        path = self.split_paths[self.file_idx]\n",
    "        smiles_list = pd.read_csv(path)['SMILES'].values\n",
    "\n",
    "        data_list = []\n",
    "        smiles_kept = []\n",
    "\n",
    "        for i, smile in enumerate(tqdm(smiles_list)):\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "            N = mol.GetNumAtoms()\n",
    "\n",
    "            type_idx = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "\n",
    "            row, col, edge_type = [], [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                edge_type += 2 * [bonds[bond.GetBondType()] + 1]\n",
    "\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "\n",
    "            edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "            edge_attr = F.one_hot(edge_type, num_classes=len(bonds) + 1).to(torch.float)\n",
    "\n",
    "            perm = (edge_index[0] * N + edge_index[1]).argsort()\n",
    "            edge_index = edge_index[:, perm]\n",
    "            edge_attr = edge_attr[perm]\n",
    "\n",
    "            x = F.one_hot(torch.tensor(type_idx), num_classes=len(types)).float()\n",
    "            y = torch.zeros(size=(1, 0), dtype=torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, idx=i)\n",
    "\n",
    "            if self.filter_dataset:\n",
    "                # Try to build the molecule again from the graph. If it fails, do not add it to the training set\n",
    "                dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                dense_data = dense_data.mask(node_mask, collapse=True)\n",
    "                X, E = dense_data.X, dense_data.E\n",
    "\n",
    "                assert X.size(0) == 1\n",
    "                atom_types = X[0]\n",
    "                edge_types = E[0]\n",
    "                mol = build_molecule_with_partial_charges(atom_types, edge_types, atom_decoder)\n",
    "                smiles = mol2smiles(mol)\n",
    "                if smiles is not None:\n",
    "                    try:\n",
    "                        mol_frags = Chem.rdmolops.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "                        if len(mol_frags) == 1:\n",
    "                            data_list.append(data)\n",
    "                            smiles_kept.append(smiles)\n",
    "\n",
    "                    except Chem.rdchem.AtomValenceException:\n",
    "                        print(\"Valence error in GetmolFrags\")\n",
    "                    except Chem.rdchem.KekulizeException:\n",
    "                        print(\"Can't kekulize molecule\")\n",
    "            else:\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "                data_list.append(data)\n",
    "\n",
    "        torch.save(self.collate(data_list), self.processed_paths[self.file_idx])\n",
    "\n",
    "        if self.filter_dataset:\n",
    "            smiles_save_path = osp.join(pathlib.Path(self.raw_paths[0]).parent, f'new_{self.stage}.smiles')\n",
    "            print(smiles_save_path)\n",
    "            with open(smiles_save_path, 'w') as f:\n",
    "                f.writelines('%s\\n' % s for s in smiles_kept)\n",
    "            print(f\"Number of molecules kept: {len(smiles_kept)} / {len(smiles_list)}\")\n",
    "\n",
    "\n",
    "\n",
    "class MosesDataModule(MolecularDataModule):\n",
    "    def __init__(self, cfg):\n",
    "        self.remove_h = False\n",
    "        self.datadir = cfg.dataset.datadir\n",
    "        self.filter_dataset = cfg.dataset.filter\n",
    "        self.train_smiles = []\n",
    "        base_path = pathlib.Path(os.path.realpath(__file__)).parents[2]\n",
    "        root_path = os.path.join(base_path, self.datadir)\n",
    "        datasets = {'train': MOSESDataset(stage='train', root=root_path, filter_dataset=self.filter_dataset),\n",
    "                    'val': MOSESDataset(stage='val', root=root_path, filter_dataset=self.filter_dataset),\n",
    "                    'test': MOSESDataset(stage='test', root=root_path, filter_dataset=self.filter_dataset)}\n",
    "        super().__init__(cfg, datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MOSESinfos(AbstractDatasetInfos):\n",
    "    def __init__(self, datamodule, cfg, recompute_statistics=False, meta=None):\n",
    "        self.name = 'MOSES'\n",
    "        self.input_dims = None\n",
    "        self.output_dims = None\n",
    "        self.remove_h = False\n",
    "\n",
    "        self.atom_decoder = atom_decoder\n",
    "        self.atom_encoder = {atom: i for i, atom in enumerate(self.atom_decoder)}\n",
    "        self.atom_weights = {0: 12, 1: 14, 2: 32, 3: 16, 4: 19, 5: 35.4, 6: 79.9, 7: 1}\n",
    "        self.valencies = [4, 3, 4, 2, 1, 1, 1, 1]\n",
    "        self.num_atom_types = len(self.atom_decoder)\n",
    "        self.max_weight = 350\n",
    "\n",
    "        meta_files = dict(n_nodes=f'{self.name}_n_counts.txt',\n",
    "                          node_types=f'{self.name}_atom_types.txt',\n",
    "                          edge_types=f'{self.name}_edge_types.txt',\n",
    "                          valency_distribution=f'{self.name}_valencies.txt')\n",
    "\n",
    "        self.n_nodes = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.097634362347889692e-06,\n",
    "                                     1.858580617408733815e-05, 5.007842264603823423e-05, 5.678996240021660924e-05,\n",
    "                                     1.244216400664299726e-04, 4.486406978685408831e-04, 2.253012731671333313e-03,\n",
    "                                     3.231865121051669121e-03, 6.709992419928312302e-03, 2.289564721286296844e-02,\n",
    "                                     5.411050841212272644e-02, 1.099515631794929504e-01, 1.223291903734207153e-01,\n",
    "                                     1.280680745840072632e-01, 1.445975750684738159e-01, 1.505961418151855469e-01,\n",
    "                                     1.436946094036102295e-01, 9.265746921300888062e-02, 1.820066757500171661e-02,\n",
    "                                     2.065089574898593128e-06])\n",
    "        self.max_n_nodes = len(self.n_nodes) - 1 if self.n_nodes is not None else None\n",
    "        self.node_types = torch.tensor([0.722338, 0.13661, 0.163655, 0.103549, 0.1421803, 0.005411, 0.00150, 0.0])\n",
    "        self.edge_types = torch.tensor([0.89740, 0.0472947, 0.062670, 0.0003524, 0.0486])\n",
    "        self.valency_distribution = torch.zeros(3 * self.max_n_nodes - 2)\n",
    "        self.valency_distribution[:7] = torch.tensor([0.0, 0.1055, 0.2728, 0.3613, 0.2499, 0.00544, 0.00485])\n",
    "\n",
    "        if meta is None:\n",
    "            meta = dict(n_nodes=None, node_types=None, edge_types=None, valency_distribution=None)\n",
    "        assert set(meta.keys()) == set(meta_files.keys())\n",
    "        for k, v in meta_files.items():\n",
    "            if (k not in meta or meta[k] is None) and os.path.exists(v):\n",
    "                meta[k] = np.loadtxt(v)\n",
    "                setattr(self, k, meta[k])\n",
    "        if recompute_statistics or self.n_nodes is None:\n",
    "            self.n_nodes = datamodule.node_counts()\n",
    "            print(\"Distribution of number of nodes\", self.n_nodes)\n",
    "            np.savetxt(meta_files[\"n_nodes\"], self.n_nodes.numpy())\n",
    "            self.max_n_nodes = len(self.n_nodes) - 1\n",
    "        if recompute_statistics or self.node_types is None:\n",
    "            self.node_types = datamodule.node_types()                                     # There are no node types\n",
    "            print(\"Distribution of node types\", self.node_types)\n",
    "            np.savetxt(meta_files[\"node_types\"], self.node_types.numpy())\n",
    "\n",
    "        if recompute_statistics or self.edge_types is None:\n",
    "            self.edge_types = datamodule.edge_counts()\n",
    "            print(\"Distribution of edge types\", self.edge_types)\n",
    "            np.savetxt(meta_files[\"edge_types\"], self.edge_types.numpy())\n",
    "        if recompute_statistics or self.valency_distribution is None:\n",
    "            valencies = datamodule.valency_count(self.max_n_nodes)\n",
    "            print(\"Distribution of the valencies\", valencies)\n",
    "            np.savetxt(meta_files[\"valency_distribution\"], valencies.numpy())\n",
    "            self.valency_distribution = valencies\n",
    "        # after we can be sure we have the data, complete infos\n",
    "        self.complete_infos(n_nodes=self.n_nodes, node_types=self.node_types)\n",
    "\n",
    "\n",
    "def get_train_smiles(cfg, datamodule, dataset_infos, evaluate_dataset=False):\n",
    "    base_path = pathlib.Path(os.path.realpath(__file__)).parents[2]\n",
    "    smiles_path = os.path.join(base_path, cfg.dataset.datadir)\n",
    "\n",
    "    train_smiles = None\n",
    "    if os.path.exists(smiles_path):\n",
    "        print(\"Dataset smiles were found.\")\n",
    "        train_smiles = np.array(open(smiles_path).readlines())\n",
    "\n",
    "    if evaluate_dataset:\n",
    "        train_dataloader = datamodule.dataloaders['train']\n",
    "        all_molecules = []\n",
    "        for i, data in enumerate(tqdm(train_dataloader)):\n",
    "            dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            dense_data = dense_data.mask(node_mask, collapse=True)\n",
    "            X, E = dense_data.X, dense_data.E\n",
    "\n",
    "            for k in range(X.size(0)):\n",
    "                n = int(torch.sum((X != -1)[k, :]))\n",
    "                atom_types = X[k, :n].cpu()\n",
    "                edge_types = E[k, :n, :n].cpu()\n",
    "                all_molecules.append([atom_types, edge_types])\n",
    "\n",
    "        print(\"Evaluating the dataset -- number of molecules to evaluate\", len(all_molecules))\n",
    "        metrics = compute_molecular_metrics(molecule_list=all_molecules, train_smiles=train_smiles,\n",
    "                                            dataset_info=dataset_infos)\n",
    "        print(metrics[0])\n",
    "\n",
    "    return train_smiles\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ds = [MOSESDataset(s, os.path.join(os.path.abspath(__file__), \"../../../data/moses\"),\n",
    "#                        preprocess=True) for s in [\"train\", \"val\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff8b935c-e562-497f-a869-55e3df63089d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PossibleUserWarning\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmolecular_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainMolecularMetrics, SamplingMolecularMetrics\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmolecular_metrics_discrete\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainMolecularMetricsDiscrete\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextra_features_molecular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtraMolecularFeatures\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metrics'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.warnings import PossibleUserWarning\n",
    "from metrics.molecular_metrics import TrainMolecularMetrics, SamplingMolecularMetrics\n",
    "from metrics.molecular_metrics_discrete import TrainMolecularMetricsDiscrete\n",
    "from diffusion.extra_features_molecular import ExtraMolecularFeatures\n",
    "from analysis.visualization import MolecularVisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cb973a4-b9aa-4195-a3e0-5d8e4617209b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DictConfig' has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from datasets import moses_dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# cfg = DictConfig\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m \u001b[43mMosesDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDictConfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 169\u001b[0m, in \u001b[0;36mMosesDataModule.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatadir \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mdatadir\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_dataset \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mfilter\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_smiles \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DictConfig' has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig\n",
    "# from datasets import moses_dataset\n",
    "# cfg = DictConfig\n",
    "datamodule = MosesDataModule(DictConfig)\n",
    "# dataset_infos = MOSESinfos(datamodule, cfg)\n",
    "# train_smiles = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80e6b62-e11f-4439-b742-34293e5d1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a812a7-b846-480c-bf56-586892782fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (digress)",
   "language": "python",
   "name": "digress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
