{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde72c77-55ab-4b8e-a595-13687b9a6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Notebook Style Implementation Codebase\n",
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.warnings import PossibleUserWarning\n",
    "\n",
    "from src import utils\n",
    "from metrics.abstract_metrics import TrainAbstractMetricsDiscrete, TrainAbstractMetrics\n",
    "\n",
    "from diffusion_model import LiftedDenoisingDiffusion\n",
    "from diffusion_model_discrete import DiscreteDenoisingDiffusion\n",
    "from diffusion.extra_features import DummyExtraFeatures, ExtraFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af18daae-0171-4726-9dde-e806c7517706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(cfg, model_kwargs):\n",
    "    \"\"\" Resumes a run. It loads previous config without allowing to update keys (used for testing). \"\"\"\n",
    "    saved_cfg = cfg.copy()\n",
    "    name = cfg.general.name + '_resume'\n",
    "    resume = cfg.general.test_only\n",
    "    if cfg.model.type == 'discrete':\n",
    "        model = DiscreteDenoisingDiffusion.load_from_checkpoint(resume, **model_kwargs)\n",
    "    else:\n",
    "        model = LiftedDenoisingDiffusion.load_from_checkpoint(resume, **model_kwargs)\n",
    "    cfg = model.cfg\n",
    "    cfg.general.test_only = resume\n",
    "    cfg.general.name = name\n",
    "    cfg = utils.update_config_with_new_keys(cfg, saved_cfg)\n",
    "    return cfg, model\n",
    "\n",
    "\n",
    "def get_resume_adaptive(cfg, model_kwargs):\n",
    "    \"\"\" Resumes a run. It loads previous config but allows to make some changes (used for resuming training).\"\"\"\n",
    "    saved_cfg = cfg.copy()\n",
    "    # Fetch path to this file to get base path\n",
    "    current_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    root_dir = current_path.split('outputs')[0]\n",
    "\n",
    "    resume_path = os.path.join(root_dir, cfg.general.resume)\n",
    "\n",
    "    if cfg.model.type == 'discrete':\n",
    "        model = DiscreteDenoisingDiffusion.load_from_checkpoint(resume_path, **model_kwargs)\n",
    "    else:\n",
    "        model = LiftedDenoisingDiffusion.load_from_checkpoint(resume_path, **model_kwargs)\n",
    "    new_cfg = model.cfg\n",
    "\n",
    "    for category in cfg:\n",
    "        for arg in cfg[category]:\n",
    "            new_cfg[category][arg] = cfg[category][arg]\n",
    "\n",
    "    new_cfg.general.resume = resume_path\n",
    "    new_cfg.general.name = new_cfg.general.name + '_resume'\n",
    "\n",
    "    new_cfg = utils.update_config_with_new_keys(new_cfg, saved_cfg)\n",
    "    return new_cfg, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac50a45-c2e3-411d-926b-90772ee5efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found rdkit, all good\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "from typing import Any, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import pandas as pd\n",
    "\n",
    "from src import utils\n",
    "from src.analysis.rdkit_functions import mol2smiles, build_molecule_with_partial_charges, compute_molecular_metrics\n",
    "from src.datasets.abstract_dataset import AbstractDatasetInfos, MolecularDataModule\n",
    "\n",
    "\n",
    "def to_list(value: Any) -> Sequence:\n",
    "    if isinstance(value, Sequence) and not isinstance(value, str):\n",
    "        return value\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "\n",
    "atom_decoder = ['C', 'N', 'S', 'O', 'F', 'Cl', 'Br', 'H']\n",
    "\n",
    "\n",
    "class MOSESDataset(InMemoryDataset):\n",
    "    train_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/train.csv'\n",
    "    val_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/test.csv'\n",
    "    test_url = 'https://media.githubusercontent.com/media/molecularsets/moses/master/data/test_scaffolds.csv'\n",
    "\n",
    "    def __init__(self, stage, root, filter_dataset: bool, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.stage = stage\n",
    "        self.atom_decoder = atom_decoder\n",
    "        self.filter_dataset = filter_dataset\n",
    "        if self.stage == 'train':\n",
    "            self.file_idx = 0\n",
    "        elif self.stage == 'val':\n",
    "            self.file_idx = 1\n",
    "        else:\n",
    "            self.file_idx = 2\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.file_idx])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train_moses.csv', 'val_moses.csv', 'test_moses.csv']\n",
    "\n",
    "    @property\n",
    "    def split_file_name(self):\n",
    "        return ['train_moses.csv', 'val_moses.csv', 'test_moses.csv']\n",
    "\n",
    "    @property\n",
    "    def split_paths(self):\n",
    "        r\"\"\"The absolute filepaths that must be present in order to skip\n",
    "        splitting.\"\"\"\n",
    "        files = to_list(self.split_file_name)\n",
    "        return [osp.join(self.raw_dir, f) for f in files]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        if self.filter_dataset:\n",
    "            return ['train_filtered.pt', 'test_filtered.pt', 'test_scaffold_filtered.pt']\n",
    "        else:\n",
    "            return ['train.pt', 'test.pt', 'test_scaffold.pt']\n",
    "\n",
    "    def download(self):\n",
    "        import rdkit  # noqa\n",
    "        train_path = download_url(self.train_url, self.raw_dir)\n",
    "        os.rename(train_path, osp.join(self.raw_dir, 'train_moses.csv'))\n",
    "\n",
    "        test_path = download_url(self.test_url, self.raw_dir)\n",
    "        os.rename(test_path, osp.join(self.raw_dir, 'val_moses.csv'))\n",
    "\n",
    "        valid_path = download_url(self.val_url, self.raw_dir)\n",
    "        os.rename(valid_path, osp.join(self.raw_dir, 'test_moses.csv'))\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "        RDLogger.DisableLog('rdApp.*')\n",
    "        types = {atom: i for i, atom in enumerate(self.atom_decoder)}\n",
    "\n",
    "        bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "\n",
    "        path = self.split_paths[self.file_idx]\n",
    "        smiles_list = pd.read_csv(path)['SMILES'].values\n",
    "\n",
    "        data_list = []\n",
    "        smiles_kept = []\n",
    "\n",
    "        for i, smile in enumerate(tqdm(smiles_list)):\n",
    "            mol = Chem.MolFromSmiles(smile)\n",
    "            N = mol.GetNumAtoms()\n",
    "\n",
    "            type_idx = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "\n",
    "            row, col, edge_type = [], [], []\n",
    "            for bond in mol.GetBonds():\n",
    "                start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                row += [start, end]\n",
    "                col += [end, start]\n",
    "                edge_type += 2 * [bonds[bond.GetBondType()] + 1]\n",
    "\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "\n",
    "            edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "            edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "            edge_attr = F.one_hot(edge_type, num_classes=len(bonds) + 1).to(torch.float)\n",
    "\n",
    "            perm = (edge_index[0] * N + edge_index[1]).argsort()\n",
    "            edge_index = edge_index[:, perm]\n",
    "            edge_attr = edge_attr[perm]\n",
    "\n",
    "            x = F.one_hot(torch.tensor(type_idx), num_classes=len(types)).float()\n",
    "            y = torch.zeros(size=(1, 0), dtype=torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, idx=i)\n",
    "\n",
    "            if self.filter_dataset:\n",
    "                # Try to build the molecule again from the graph. If it fails, do not add it to the training set\n",
    "                dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                dense_data = dense_data.mask(node_mask, collapse=True)\n",
    "                X, E = dense_data.X, dense_data.E\n",
    "\n",
    "                assert X.size(0) == 1\n",
    "                atom_types = X[0]\n",
    "                edge_types = E[0]\n",
    "                mol = build_molecule_with_partial_charges(atom_types, edge_types, atom_decoder)\n",
    "                smiles = mol2smiles(mol)\n",
    "                if smiles is not None:\n",
    "                    try:\n",
    "                        mol_frags = Chem.rdmolops.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "                        if len(mol_frags) == 1:\n",
    "                            data_list.append(data)\n",
    "                            smiles_kept.append(smiles)\n",
    "\n",
    "                    except Chem.rdchem.AtomValenceException:\n",
    "                        print(\"Valence error in GetmolFrags\")\n",
    "                    except Chem.rdchem.KekulizeException:\n",
    "                        print(\"Can't kekulize molecule\")\n",
    "            else:\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "                data_list.append(data)\n",
    "\n",
    "        torch.save(self.collate(data_list), self.processed_paths[self.file_idx])\n",
    "\n",
    "        if self.filter_dataset:\n",
    "            smiles_save_path = osp.join(pathlib.Path(self.raw_paths[0]).parent, f'new_{self.stage}.smiles')\n",
    "            print(smiles_save_path)\n",
    "            with open(smiles_save_path, 'w') as f:\n",
    "                f.writelines('%s\\n' % s for s in smiles_kept)\n",
    "            print(f\"Number of molecules kept: {len(smiles_kept)} / {len(smiles_list)}\")\n",
    "\n",
    "\n",
    "\n",
    "class MosesDataModule(MolecularDataModule):\n",
    "    def __init__(self, cfg):\n",
    "        self.remove_h = False\n",
    "        self.datadir = cfg.dataset.datadir\n",
    "        # self.filter_dataset = cfg.dataset.filter\n",
    "        self.filter_dataset = cfg.dataset.get(\"filter\", None)  # Safe access to 'filter'\n",
    "        # self.filter_dataset = cfg.dataset.get(\"filter\", None)  # Default to None if missing\n",
    "        self.train_smiles = []\n",
    "        base_path = pathlib.Path(os.path.realpath(__file__)).parents[2]\n",
    "        root_path = os.path.join(base_path, self.datadir)\n",
    "        datasets = {'train': MOSESDataset(stage='train', root=root_path, filter_dataset=self.filter_dataset),\n",
    "                    'val': MOSESDataset(stage='val', root=root_path, filter_dataset=self.filter_dataset),\n",
    "                    'test': MOSESDataset(stage='test', root=root_path, filter_dataset=self.filter_dataset)}\n",
    "        super().__init__(cfg, datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MOSESinfos(AbstractDatasetInfos):\n",
    "    def __init__(self, datamodule, cfg, recompute_statistics=False, meta=None):\n",
    "        self.name = 'MOSES'\n",
    "        self.input_dims = None\n",
    "        self.output_dims = None\n",
    "        self.remove_h = False\n",
    "\n",
    "        self.atom_decoder = atom_decoder\n",
    "        self.atom_encoder = {atom: i for i, atom in enumerate(self.atom_decoder)}\n",
    "        self.atom_weights = {0: 12, 1: 14, 2: 32, 3: 16, 4: 19, 5: 35.4, 6: 79.9, 7: 1}\n",
    "        self.valencies = [4, 3, 4, 2, 1, 1, 1, 1]\n",
    "        self.num_atom_types = len(self.atom_decoder)\n",
    "        self.max_weight = 350\n",
    "\n",
    "        meta_files = dict(n_nodes=f'{self.name}_n_counts.txt',\n",
    "                          node_types=f'{self.name}_atom_types.txt',\n",
    "                          edge_types=f'{self.name}_edge_types.txt',\n",
    "                          valency_distribution=f'{self.name}_valencies.txt')\n",
    "\n",
    "        self.n_nodes = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.097634362347889692e-06,\n",
    "                                     1.858580617408733815e-05, 5.007842264603823423e-05, 5.678996240021660924e-05,\n",
    "                                     1.244216400664299726e-04, 4.486406978685408831e-04, 2.253012731671333313e-03,\n",
    "                                     3.231865121051669121e-03, 6.709992419928312302e-03, 2.289564721286296844e-02,\n",
    "                                     5.411050841212272644e-02, 1.099515631794929504e-01, 1.223291903734207153e-01,\n",
    "                                     1.280680745840072632e-01, 1.445975750684738159e-01, 1.505961418151855469e-01,\n",
    "                                     1.436946094036102295e-01, 9.265746921300888062e-02, 1.820066757500171661e-02,\n",
    "                                     2.065089574898593128e-06])\n",
    "        self.max_n_nodes = len(self.n_nodes) - 1 if self.n_nodes is not None else None\n",
    "        self.node_types = torch.tensor([0.722338, 0.13661, 0.163655, 0.103549, 0.1421803, 0.005411, 0.00150, 0.0])\n",
    "        self.edge_types = torch.tensor([0.89740, 0.0472947, 0.062670, 0.0003524, 0.0486])\n",
    "        self.valency_distribution = torch.zeros(3 * self.max_n_nodes - 2)\n",
    "        self.valency_distribution[:7] = torch.tensor([0.0, 0.1055, 0.2728, 0.3613, 0.2499, 0.00544, 0.00485])\n",
    "\n",
    "        if meta is None:\n",
    "            meta = dict(n_nodes=None, node_types=None, edge_types=None, valency_distribution=None)\n",
    "        assert set(meta.keys()) == set(meta_files.keys())\n",
    "        for k, v in meta_files.items():\n",
    "            if (k not in meta or meta[k] is None) and os.path.exists(v):\n",
    "                meta[k] = np.loadtxt(v)\n",
    "                setattr(self, k, meta[k])\n",
    "        if recompute_statistics or self.n_nodes is None:\n",
    "            self.n_nodes = datamodule.node_counts()\n",
    "            print(\"Distribution of number of nodes\", self.n_nodes)\n",
    "            np.savetxt(meta_files[\"n_nodes\"], self.n_nodes.numpy())\n",
    "            self.max_n_nodes = len(self.n_nodes) - 1\n",
    "        if recompute_statistics or self.node_types is None:\n",
    "            self.node_types = datamodule.node_types()                                     # There are no node types\n",
    "            print(\"Distribution of node types\", self.node_types)\n",
    "            np.savetxt(meta_files[\"node_types\"], self.node_types.numpy())\n",
    "\n",
    "        if recompute_statistics or self.edge_types is None:\n",
    "            self.edge_types = datamodule.edge_counts()\n",
    "            print(\"Distribution of edge types\", self.edge_types)\n",
    "            np.savetxt(meta_files[\"edge_types\"], self.edge_types.numpy())\n",
    "        if recompute_statistics or self.valency_distribution is None:\n",
    "            valencies = datamodule.valency_count(self.max_n_nodes)\n",
    "            print(\"Distribution of the valencies\", valencies)\n",
    "            np.savetxt(meta_files[\"valency_distribution\"], valencies.numpy())\n",
    "            self.valency_distribution = valencies\n",
    "        # after we can be sure we have the data, complete infos\n",
    "        self.complete_infos(n_nodes=self.n_nodes, node_types=self.node_types)\n",
    "\n",
    "\n",
    "def get_train_smiles(cfg, datamodule, dataset_infos, evaluate_dataset=False):\n",
    "    base_path = pathlib.Path(os.path.realpath(__file__)).parents[2]\n",
    "    smiles_path = os.path.join(base_path, cfg.dataset.datadir)\n",
    "\n",
    "    train_smiles = None\n",
    "    if os.path.exists(smiles_path):\n",
    "        print(\"Dataset smiles were found.\")\n",
    "        train_smiles = np.array(open(smiles_path).readlines())\n",
    "\n",
    "    if evaluate_dataset:\n",
    "        train_dataloader = datamodule.dataloaders['train']\n",
    "        all_molecules = []\n",
    "        for i, data in enumerate(tqdm(train_dataloader)):\n",
    "            dense_data, node_mask = utils.to_dense(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            dense_data = dense_data.mask(node_mask, collapse=True)\n",
    "            X, E = dense_data.X, dense_data.E\n",
    "\n",
    "            for k in range(X.size(0)):\n",
    "                n = int(torch.sum((X != -1)[k, :]))\n",
    "                atom_types = X[k, :n].cpu()\n",
    "                edge_types = E[k, :n, :n].cpu()\n",
    "                all_molecules.append([atom_types, edge_types])\n",
    "\n",
    "        print(\"Evaluating the dataset -- number of molecules to evaluate\", len(all_molecules))\n",
    "        metrics = compute_molecular_metrics(molecule_list=all_molecules, train_smiles=train_smiles,\n",
    "                                            dataset_info=dataset_infos)\n",
    "        print(metrics[0])\n",
    "\n",
    "    return train_smiles\n",
    "\n",
    "# This is for downloading new moses dataset\n",
    "# if __name__ == \"__main__\":\n",
    "#     ds = [MOSESDataset(s, os.path.join(os.getcwd(), \"../../../data/moses\"), filter_dataset=True) \n",
    "#           for s in [\"train\", \"val\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b81561b-9563-42d1-9126-da45d116c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the Yaml File under this directory for model selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67959d7d-cbcd-4fd5-b116-6aa7100a1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general:\n",
      "  name: graph-tf-model\n",
      "  wandb: online\n",
      "  gpus: 1\n",
      "  resume: null\n",
      "  test_only: null\n",
      "  check_val_every_n_epochs: 5\n",
      "  sample_every_val: 4\n",
      "  val_check_interval: null\n",
      "  samples_to_generate: 512\n",
      "  samples_to_save: 20\n",
      "  chains_to_save: 1\n",
      "  log_every_steps: 50\n",
      "  number_chain_steps: 50\n",
      "  final_model_samples_to_generate: 10000\n",
      "  final_model_samples_to_save: 30\n",
      "  final_model_chains_to_save: 20\n",
      "  evaluate_all_checkpoints: false\n",
      "model:\n",
      "  type: continuous\n",
      "  model: graph_tf\n",
      "  diffusion_steps: 500\n",
      "  diffusion_noise_schedule: cosine\n",
      "  n_layers: 6\n",
      "  hidden_mlp_dims:\n",
      "    X: 256\n",
      "    E: 128\n",
      "    'y': 128\n",
      "  hidden_dims:\n",
      "    dx: 256\n",
      "    de: 64\n",
      "    dy: 64\n",
      "    n_head: 8\n",
      "    dim_ffX: 256\n",
      "    dim_ffE: 128\n",
      "    dim_ffy: 128\n",
      "  normalize_factors:\n",
      "  - 2\n",
      "  - 1\n",
      "  - 1\n",
      "  norm_biases:\n",
      "  - 0\n",
      "  - 0\n",
      "  - 0\n",
      "  lambda_train:\n",
      "  - 5\n",
      "  - 0\n",
      "train:\n",
      "  n_epochs: 1000\n",
      "  batch_size: 512\n",
      "  lr: 0.0002\n",
      "  clip_grad: null\n",
      "  save_model: true\n",
      "  num_workers: 0\n",
      "  ema_decay: 0\n",
      "  progress_bar: false\n",
      "  weight_decay: 1.0e-12\n",
      "  optimizer: adamw\n",
      "  seed: 0\n",
      "dataset:\n",
      "  name: moses\n",
      "  datadir: data/moses/moses_pyg/\n",
      "  remove_h: null\n",
      "  filter: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "hydra.initialize(config_path=\"../configs\", version_base=\"1.3\")  # Initialize first\n",
    "cfg = hydra.compose(config_name=\"config\")  # Then compose the config\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))  # Print to check if cfg is loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a497fb-721f-4a02-9a89-bec2b981e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moses Dataset Preparing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "  1%|▏         | 23398/1584663 [00:17<19:39, 1323.15it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.warnings import PossibleUserWarning\n",
    "\n",
    "from src import utils\n",
    "from metrics.abstract_metrics import TrainAbstractMetricsDiscrete, TrainAbstractMetrics\n",
    "\n",
    "from diffusion_model import LiftedDenoisingDiffusion\n",
    "from diffusion_model_discrete import DiscreteDenoisingDiffusion\n",
    "from diffusion.extra_features import DummyExtraFeatures, ExtraFeatures\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=PossibleUserWarning)\n",
    "\n",
    "\n",
    "def get_resume(cfg, model_kwargs):\n",
    "    \"\"\" Resumes a run. It loads previous config without allowing to update keys (used for testing). \"\"\"\n",
    "    saved_cfg = cfg.copy()\n",
    "    name = cfg.general.name + '_resume'\n",
    "    resume = cfg.general.test_only\n",
    "    if cfg.model.type == 'discrete':\n",
    "        model = DiscreteDenoisingDiffusion.load_from_checkpoint(resume, **model_kwargs)\n",
    "    else:\n",
    "        model = LiftedDenoisingDiffusion.load_from_checkpoint(resume, **model_kwargs)\n",
    "    cfg = model.cfg\n",
    "    cfg.general.test_only = resume\n",
    "    cfg.general.name = name\n",
    "    cfg = utils.update_config_with_new_keys(cfg, saved_cfg)\n",
    "    return cfg, model\n",
    "\n",
    "\n",
    "def get_resume_adaptive(cfg, model_kwargs):\n",
    "    \"\"\" Resumes a run. It loads previous config but allows to make some changes (used for resuming training).\"\"\"\n",
    "    saved_cfg = cfg.copy()\n",
    "    # Fetch path to this file to get base path\n",
    "    current_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    root_dir = current_path.split('outputs')[0]\n",
    "\n",
    "    resume_path = os.path.join(root_dir, cfg.general.resume)\n",
    "\n",
    "    if cfg.model.type == 'discrete':\n",
    "        model = DiscreteDenoisingDiffusion.load_from_checkpoint(resume_path, **model_kwargs)\n",
    "    else:\n",
    "        model = LiftedDenoisingDiffusion.load_from_checkpoint(resume_path, **model_kwargs)\n",
    "    new_cfg = model.cfg\n",
    "\n",
    "    for category in cfg:\n",
    "        for arg in cfg[category]:\n",
    "            new_cfg[category][arg] = cfg[category][arg]\n",
    "\n",
    "    new_cfg.general.resume = resume_path\n",
    "    new_cfg.general.name = new_cfg.general.name + '_resume'\n",
    "\n",
    "    new_cfg = utils.update_config_with_new_keys(new_cfg, saved_cfg)\n",
    "    return new_cfg, model\n",
    "\n",
    "\n",
    "\n",
    "@hydra.main(version_base='1.3', config_path='../configs', config_name='config')\n",
    "def main(cfg: DictConfig):\n",
    "    dataset_config = cfg[\"dataset\"]\n",
    "\n",
    "    if dataset_config[\"name\"] in ['sbm', 'comm20', 'planar']:\n",
    "        from datasets.spectre_dataset import SpectreGraphDataModule, SpectreDatasetInfos\n",
    "        from analysis.spectre_utils import PlanarSamplingMetrics, SBMSamplingMetrics, Comm20SamplingMetrics\n",
    "        from analysis.visualization import NonMolecularVisualization\n",
    "\n",
    "        datamodule = SpectreGraphDataModule(cfg)\n",
    "        if dataset_config['name'] == 'sbm':\n",
    "            sampling_metrics = SBMSamplingMetrics(datamodule)\n",
    "        elif dataset_config['name'] == 'comm20':\n",
    "            sampling_metrics = Comm20SamplingMetrics(datamodule)\n",
    "        else:\n",
    "            sampling_metrics = PlanarSamplingMetrics(datamodule)\n",
    "\n",
    "        dataset_infos = SpectreDatasetInfos(datamodule, dataset_config)\n",
    "        train_metrics = TrainAbstractMetricsDiscrete() if cfg.model.type == 'discrete' else TrainAbstractMetrics()\n",
    "        visualization_tools = NonMolecularVisualization()\n",
    "\n",
    "        if cfg.model.type == 'discrete' and cfg.model.extra_features is not None:\n",
    "            extra_features = ExtraFeatures(cfg.model.extra_features, dataset_info=dataset_infos)\n",
    "        else:\n",
    "            extra_features = DummyExtraFeatures()\n",
    "        domain_features = DummyExtraFeatures()\n",
    "\n",
    "        dataset_infos.compute_input_output_dims(datamodule=datamodule, extra_features=extra_features,\n",
    "                                                domain_features=domain_features)\n",
    "\n",
    "        model_kwargs = {'dataset_infos': dataset_infos, 'train_metrics': train_metrics,\n",
    "                        'sampling_metrics': sampling_metrics, 'visualization_tools': visualization_tools,\n",
    "                        'extra_features': extra_features, 'domain_features': domain_features}\n",
    "\n",
    "    elif dataset_config[\"name\"] in ['qm9', 'guacamol', 'moses']:\n",
    "        from metrics.molecular_metrics import TrainMolecularMetrics, SamplingMolecularMetrics\n",
    "        from metrics.molecular_metrics_discrete import TrainMolecularMetricsDiscrete\n",
    "        from diffusion.extra_features_molecular import ExtraMolecularFeatures\n",
    "        from analysis.visualization import MolecularVisualization\n",
    "\n",
    "        if dataset_config[\"name\"] == 'qm9':\n",
    "            from datasets import qm9_dataset\n",
    "            datamodule = qm9_dataset.QM9DataModule(cfg)\n",
    "            dataset_infos = qm9_dataset.QM9infos(datamodule=datamodule, cfg=cfg)\n",
    "            train_smiles = qm9_dataset.get_train_smiles(cfg=cfg, train_dataloader=datamodule.train_dataloader(),\n",
    "                                                        dataset_infos=dataset_infos, evaluate_dataset=False)\n",
    "        elif dataset_config['name'] == 'guacamol':\n",
    "            from datasets import guacamol_dataset\n",
    "            datamodule = guacamol_dataset.GuacamolDataModule(cfg)\n",
    "            dataset_infos = guacamol_dataset.Guacamolinfos(datamodule, cfg)\n",
    "            train_smiles = None\n",
    "\n",
    "        elif dataset_config.name == 'moses':\n",
    "            print(\"Moses Dataset Preparing...\")\n",
    "            from datasets import moses_dataset\n",
    "            datamodule = moses_dataset.MosesDataModule(cfg)\n",
    "            dataset_infos = moses_dataset.MOSESinfos(datamodule, cfg)\n",
    "            train_smiles = None\n",
    "            print(\"Moses Dataset Preparing Done!\")\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not implemented\")\n",
    "\n",
    "        if cfg.model.type == 'discrete' and cfg.model.extra_features is not None:\n",
    "            extra_features = ExtraFeatures(cfg.model.extra_features, dataset_info=dataset_infos)\n",
    "            domain_features = ExtraMolecularFeatures(dataset_infos=dataset_infos)\n",
    "        else:\n",
    "            extra_features = DummyExtraFeatures()\n",
    "            domain_features = DummyExtraFeatures()\n",
    "\n",
    "        dataset_infos.compute_input_output_dims(datamodule=datamodule, extra_features=extra_features,\n",
    "                                                domain_features=domain_features)\n",
    "\n",
    "        if cfg.model.type == 'discrete':\n",
    "            train_metrics = TrainMolecularMetricsDiscrete(dataset_infos)\n",
    "        else:\n",
    "            train_metrics = TrainMolecularMetrics(dataset_infos)\n",
    "\n",
    "        # We do not evaluate novelty during training\n",
    "        sampling_metrics = SamplingMolecularMetrics(dataset_infos, train_smiles)\n",
    "        visualization_tools = MolecularVisualization(cfg.dataset.remove_h, dataset_infos=dataset_infos)\n",
    "\n",
    "        model_kwargs = {'dataset_infos': dataset_infos, 'train_metrics': train_metrics,\n",
    "                        'sampling_metrics': sampling_metrics, 'visualization_tools': visualization_tools,\n",
    "                        'extra_features': extra_features, 'domain_features': domain_features}\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown dataset {}\".format(cfg[\"dataset\"]))\n",
    "\n",
    "    if cfg.general.test_only:\n",
    "        # When testing, previous configuration is fully loaded\n",
    "        cfg, _ = get_resume(cfg, model_kwargs)\n",
    "        os.chdir(cfg.general.test_only.split('checkpoints')[0])\n",
    "    elif cfg.general.resume is not None:\n",
    "        # When resuming, we can override some parts of previous configuration\n",
    "        cfg, _ = get_resume_adaptive(cfg, model_kwargs)\n",
    "        os.chdir(cfg.general.resume.split('checkpoints')[0])\n",
    "\n",
    "    utils.create_folders(cfg)\n",
    "\n",
    "    if cfg.model.type == 'discrete':\n",
    "        model = DiscreteDenoisingDiffusion(cfg=cfg, **model_kwargs)\n",
    "    else:\n",
    "        model = LiftedDenoisingDiffusion(cfg=cfg, **model_kwargs)\n",
    "\n",
    "    # if cfg.model.typ == 'gin':\n",
    "    #     model = # TODO\n",
    "    # else:\n",
    "    #     model = LiftedDenoisingDiffusion(cfg = cfg, **model_kwargs)\n",
    "    callbacks = []\n",
    "    if cfg.train.save_model:\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=f\"checkpoints/{cfg.general.name}\",\n",
    "                                              filename='{epoch}',\n",
    "                                              monitor='val/epoch_NLL',\n",
    "                                              save_top_k=5,\n",
    "                                              mode='min',\n",
    "                                              every_n_epochs=1)\n",
    "        last_ckpt_save = ModelCheckpoint(dirpath=f\"checkpoints/{cfg.general.name}\", filename='last', every_n_epochs=1)\n",
    "        callbacks.append(last_ckpt_save)\n",
    "        callbacks.append(checkpoint_callback)\n",
    "\n",
    "    if cfg.train.ema_decay > 0:\n",
    "        ema_callback = utils.EMA(decay=cfg.train.ema_decay)\n",
    "        callbacks.append(ema_callback)\n",
    "\n",
    "    name = cfg.general.name\n",
    "    if name == 'debug':\n",
    "        print(\"[WARNING]: Run is called 'debug' -- it will run with fast_dev_run. \")\n",
    "\n",
    "    print(\"Setting GPU\")\n",
    "    use_gpu = cfg.general.gpus > 0 and torch.cuda.is_available()\n",
    "    trainer = Trainer(gradient_clip_val=cfg.train.clip_grad,\n",
    "                      strategy=\"ddp_find_unused_parameters_true\",  # Needed to load old checkpoints\n",
    "                      accelerator='gpu' if use_gpu else 'cpu',\n",
    "                      devices=cfg.general.gpus if use_gpu else 1,\n",
    "                      max_epochs=cfg.train.n_epochs,\n",
    "                      check_val_every_n_epoch=cfg.general.check_val_every_n_epochs,\n",
    "                      fast_dev_run=cfg.general.name == 'debug',\n",
    "                      enable_progress_bar=False,\n",
    "                      callbacks=callbacks,\n",
    "                      log_every_n_steps=50 if name != 'debug' else 1,\n",
    "                      logger = [])\n",
    "\n",
    "    print(\"Model Starts to Train\")\n",
    "    if not cfg.general.test_only:\n",
    "        trainer.fit(model, datamodule=datamodule, ckpt_path=cfg.general.resume)\n",
    "        if cfg.general.name not in ['debug', 'test']:\n",
    "            trainer.test(model, datamodule=datamodule)\n",
    "    else:\n",
    "        # Start by evaluating test_only_path\n",
    "        trainer.test(model, datamodule=datamodule, ckpt_path=cfg.general.test_only)\n",
    "        if cfg.general.evaluate_all_checkpoints:\n",
    "            directory = pathlib.Path(cfg.general.test_only).parents[0]\n",
    "            print(\"Directory:\", directory)\n",
    "            files_list = os.listdir(directory)\n",
    "            for file in files_list:\n",
    "                if '.ckpt' in file:\n",
    "                    ckpt_path = os.path.join(directory, file)\n",
    "                    if ckpt_path == cfg.general.test_only:\n",
    "                        continue\n",
    "                    print(\"Loading checkpoint\", ckpt_path)\n",
    "                    trainer.test(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e807e-b6f4-404a-8b65-a030d50142f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (digress)",
   "language": "python",
   "name": "digress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
